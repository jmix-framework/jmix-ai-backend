from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from rouge_score import rouge_scorer
from bert_score import score
from typing import Dict

# Define the input model for the evaluation request
class EvaluationInput(BaseModel):
    reference: str
    actual: str

# AnswerChecks class for evaluating answers
class AnswerChecks:
    def __init__(self):
        """
        Initialize the AnswerChecks service with a ROUGE scorer for ROUGE-1, ROUGE-2, and ROUGE-L.
        Stemming is enabled to improve word matching.
        """
        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    
    def evaluate_rouge(self, reference: str, actual: str) -> Dict[str, Dict[str, float]]:
        """
        Evaluate the quality of the actual answer compared to the reference answer.

        Args:
            reference (str): The reference answer.
            actual (str): The actual answer generated by the AI.

        Returns:
            Dict[str, Dict[str, float]]: A dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L scores,
            each with precision, recall, and f1 scores.
        """
        reference = reference.lower()
        actual = actual.lower()
        scores = self.scorer.score(reference, actual)
        return {key: {'precision': value.precision, 'recall': value.recall, 'f1': value.fmeasure} for key, value in scores.items()}

    def evaluate_bert(self, reference: str, actual: str, model_type: str = "bert-base-uncased") -> Dict[str, float]:
        """
        Evaluate the quality of the actual answer compared to the reference answer using BERTScore.

        Args:
            reference (str): The reference answer.
            actual (str): The actual answer generated by the AI.
            model_type (str): The BERT model to use (e.g., 'bert-base-uncased', 'roberta-large').

        Returns:
            Dict[str, float]: A dictionary containing BERTScore precision, recall, and f1 scores.
        """
        reference = reference.lower()
        actual = actual.lower()
        P, R, F1 = score([actual], [reference], lang="en", model_type=model_type, rescale_with_baseline=False)
        return {
            'precision': float(P[0]),
            'recall': float(R[0]),
            'f1': float(F1[0])
        }

# Initialize FastAPI app
app = FastAPI()

# Root endpoint
@app.get("/")
async def root():
    """
    Root endpoint to confirm the service is running.
    """
    return {"message": "AnswerChecks Service is running."}

# ROUGE evaluation endpoint
@app.post("/rouge")
async def evaluate_rouge(input_data: EvaluationInput):
    """
    Endpoint to evaluate answer quality using ROUGE metrics.

    Expects a JSON payload with 'reference' and 'actual' keys, both containing strings.

    Returns:
        JSON response with ROUGE-1, ROUGE-2, and ROUGE-L scores, or an error message if input is invalid.
    """
    if not input_data.reference or not input_data.actual:
        raise HTTPException(status_code=400, detail="Missing reference or actual in request body")
    checker = AnswerChecks()
    scores = checker.evaluate_rouge(input_data.reference, input_data.actual)
    return scores

# BERTScore evaluation endpoint
@app.post("/bert")
async def evaluate_bert(input_data: EvaluationInput):
    """
    Endpoint to evaluate answer quality using BERTScore metrics.

    Expects a JSON payload with 'reference' and 'actual' keys, both containing strings.

    Returns:
        JSON response with BERTScore precision, recall, and f1 scores, or an error message if input is invalid.
    """
    if not input_data.reference or not input_data.actual:
        raise HTTPException(status_code=400, detail="Missing reference or actual in request body")
    checker = AnswerChecks()
    scores = checker.evaluate_bert(input_data.reference, input_data.actual, model_type="bert-base-uncased")
    return scores

@app.get("/health")
async def health():
    return {"status": "healthy"}